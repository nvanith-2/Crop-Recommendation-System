{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Guassian Naive Bayes model"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:31.894924Z","iopub.status.busy":"2024-04-19T16:40:31.894407Z","iopub.status.idle":"2024-04-19T16:40:31.901723Z","shell.execute_reply":"2024-04-19T16:40:31.900191Z","shell.execute_reply.started":"2024-04-19T16:40:31.894891Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","\n","DATA_PATH=\".\\\\input\\\\Crop_recommendation.csv\"\n"]},{"cell_type":"markdown","metadata":{},"source":["1.  Importing necessary Libraries(numpy and pandas).\n","2.  Specifying the path to the datset."]},{"cell_type":"markdown","metadata":{},"source":["### read the data_path and display dataframe"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:36.910295Z","iopub.status.busy":"2024-04-19T16:40:36.909902Z","iopub.status.idle":"2024-04-19T16:40:36.969051Z","shell.execute_reply":"2024-04-19T16:40:36.967801Z","shell.execute_reply.started":"2024-04-19T16:40:36.910266Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N</th>\n","      <th>P</th>\n","      <th>K</th>\n","      <th>temperature</th>\n","      <th>humidity</th>\n","      <th>ph</th>\n","      <th>rainfall</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>90</td>\n","      <td>42</td>\n","      <td>43</td>\n","      <td>20.879744</td>\n","      <td>82.002744</td>\n","      <td>6.502985</td>\n","      <td>202.935536</td>\n","      <td>rice</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>85</td>\n","      <td>58</td>\n","      <td>41</td>\n","      <td>21.770462</td>\n","      <td>80.319644</td>\n","      <td>7.038096</td>\n","      <td>226.655537</td>\n","      <td>rice</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60</td>\n","      <td>55</td>\n","      <td>44</td>\n","      <td>23.004459</td>\n","      <td>82.320763</td>\n","      <td>7.840207</td>\n","      <td>263.964248</td>\n","      <td>rice</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>74</td>\n","      <td>35</td>\n","      <td>40</td>\n","      <td>26.491096</td>\n","      <td>80.158363</td>\n","      <td>6.980401</td>\n","      <td>242.864034</td>\n","      <td>rice</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>78</td>\n","      <td>42</td>\n","      <td>42</td>\n","      <td>20.130175</td>\n","      <td>81.604873</td>\n","      <td>7.628473</td>\n","      <td>262.717340</td>\n","      <td>rice</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    N   P   K  temperature   humidity        ph    rainfall label\n","0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n","1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n","2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n","3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n","4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df=pd.read_csv(DATA_PATH)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["1. Load the dataset from the specified path.\n","2. Display the first few rows of the dataset- 'head()'."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:39.255417Z","iopub.status.busy":"2024-04-19T16:40:39.254432Z","iopub.status.idle":"2024-04-19T16:40:39.266988Z","shell.execute_reply":"2024-04-19T16:40:39.265640Z","shell.execute_reply.started":"2024-04-19T16:40:39.255353Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['apple' 'banana' 'blackgram' 'chickpea' 'coconut' 'coffee' 'cotton'\n"," 'grapes' 'jute' 'kidneybeans' 'lentil' 'maize' 'mango' 'mothbeans'\n"," 'mungbean' 'muskmelon' 'orange' 'papaya' 'pigeonpeas' 'pomegranate'\n"," 'rice' 'watermelon']\n"]}],"source":["Y = np.asarray(df['label'])\n","unique_ele = np.unique(Y)\n","print(unique_ele) # Printing the unique crop types to view"]},{"cell_type":"markdown","metadata":{},"source":["1. Seperating the Labels into a numpy array.\n","2. Convert the 'label' coloumn of dataframe to a numpy array.\n","3. Identify the unique elements (crop types) in the 'label' array.\n","4. Now 'Y' contains the labels, and 'unique_ele' contains the distinct crop types.\n","5. These will be useful for building and evaluating the model."]},{"cell_type":"markdown","metadata":{},"source":["### Indexing the each distinct label"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:41.926970Z","iopub.status.busy":"2024-04-19T16:40:41.926540Z","iopub.status.idle":"2024-04-19T16:40:41.958276Z","shell.execute_reply":"2024-04-19T16:40:41.957089Z","shell.execute_reply.started":"2024-04-19T16:40:41.926937Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[20 20 20 ... 5 5 5]\n"]}],"source":["for i in range(len(Y)):\n","    for j in range(len(unique_ele)):\n","        if Y[i] == unique_ele[j]:\n","            Y[i] = j\n","print(Y)"]},{"cell_type":"markdown","metadata":{},"source":["1. Orginal crop type label replaced by its corresponding index in the unique list of crop types.\n","2. converting categorical labels (such as crop types) into numerical indices, we make it easier for the model to process and learn from the data."]},{"cell_type":"markdown","metadata":{},"source":["## Train-Test data split"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:47.778381Z","iopub.status.busy":"2024-04-19T16:40:47.777763Z","iopub.status.idle":"2024-04-19T16:40:47.785740Z","shell.execute_reply":"2024-04-19T16:40:47.784629Z","shell.execute_reply.started":"2024-04-19T16:40:47.778330Z"},"trusted":true},"outputs":[],"source":["def train_test_split(seed,split,X,Y):\n","    np.random.seed(seed)\n","    indices = np.random.permutation(len(df))\n","    \n","    # Define the test size\n","    test_size = split\n","    num_test_samples = int(len(df) * test_size)\n","    \n","    # Split indices into train and test\n","    train_indices = indices[:-num_test_samples]\n","    test_indices = indices[-num_test_samples:]\n","    x_train = X.iloc[train_indices].values\n","    y_train = Y[train_indices]\n","    x_test = X.iloc[test_indices].values\n","    y_test = Y[test_indices]\n","    \n","    return x_train,y_train,x_test,y_test\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["1. Sets the random seed to ensure reproducibility in random number generation.Generates a random permutation of indices corresponding to the rows of the DataFrame \n","2. Calculate the test size:  The split parameter determines the fraction of data to be allocated for testing\n","3. Split the indices into training and testing sets\n","4. Extract the corresponding data for training and testing\n","5. Returns the training and testing data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:50.810414Z","iopub.status.busy":"2024-04-19T16:40:50.809992Z","iopub.status.idle":"2024-04-19T16:40:50.828518Z","shell.execute_reply":"2024-04-19T16:40:50.827183Z","shell.execute_reply.started":"2024-04-19T16:40:50.810381Z"},"trusted":true},"outputs":[],"source":[" X=df.drop('label',axis=1).copy()\n"," x_train,y_train,x_test,y_test=train_test_split(seed=20,split=0.2,X=X,Y=Y)"]},{"cell_type":"markdown","metadata":{},"source":["1. sets the seed to 20 for random generation\n","2. split size = 0.2 i.e., test data \n","3. X is dataframe excluding the 'label' column"]},{"cell_type":"markdown","metadata":{},"source":["## Gaussian Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{},"source":["1.  **A Gaussian Naive Bayes (GNB) classifier is a type of Naive Bayes classifier that assumes the features follow a Gaussian (normal) distribution.**\n","1.  **Naive Bayes Classification: Like other Naive Bayes classifiers, GNB calculates the probability of a data point belonging to each class and then assigns the data point to the class with the highest probability.**\n","1. **Training Phase: During training, GNB calculates and stores**\n","    1. **Class probabilities: The likelihood of each class occurring in the dataset.**\n","    2. **Mean and variance of features for each class: These parameters are used to model the Gaussian distribution for each class.**\n","1. **Prediction Phase:**\n","    1. **Given a new data point with feature values, GNB calculates the probability of the data point belonging to each class using Bayes' theorem.**\n","    2. **Bayes' theorem combines the prior probability (class probabilities) with the likelihood (probability of feature values given the class) to compute the posterior probability (probability of the class given the feature values).**\n","    3. **The class with the highest posterior probability is predicted as the class for the new data point.**\n","    "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:53.839518Z","iopub.status.busy":"2024-04-19T16:40:53.839080Z","iopub.status.idle":"2024-04-19T16:40:53.854739Z","shell.execute_reply":"2024-04-19T16:40:53.853257Z","shell.execute_reply.started":"2024-04-19T16:40:53.839486Z"},"trusted":true},"outputs":[],"source":["class GaussianNaiveBayes:\n","    def fit(self, X_train, y_train):\n","        self.classes = np.unique(y_train)\n","        self.class_probs = {}\n","        self.means = {}\n","        self.vars = {}\n","        \n","        for c in self.classes:\n","            indices = np.where(y_train == c)[0]\n","            X_c = X_train[indices]\n","            self.class_probs[c] = len(X_c) / len(X_train)\n","            self.means[c] = np.mean(X_c, axis=0)\n","            self.vars[c] = np.var(X_c, axis=0)\n","            print(self.class_probs[c])\n","    \n","    def predict(self, X_test):\n","         predictions = []\n","         for x in X_test:\n","             posteriors = []\n","             for c in self.classes:\n","                 prior = np.log(self.class_probs[c])\n","                 likelihood = np.sum(np.log(self.pdf(x, self.means[c], self.vars[c])))\n","                 posterior = prior + likelihood\n","                 posteriors.append(posterior)\n","             predicted_class = self.classes[np.argmax(posteriors)]\n","             predictions.append(predicted_class)\n","         return predictions\n","\n","\n","    \n","    def pdf(self, x, mean, var):\n","        sqrt_var = np.sqrt(var)\n","        if np.any(sqrt_var == 0):\n","            return 0  # Handle division by zero case\n","        else:\n","            exponent = -np.sum((x - mean)**2 / (2 * var))\n","            pdf = np.exp(exponent) / ((2 * np.pi) ** (len(x) / 2) * np.prod(sqrt_var))\n","            return pdf\n"]},{"cell_type":"markdown","metadata":{},"source":["1. This code defines a class GaussianNaiveBayes implementing a Gaussian Naive Bayes classifier. \n","2. fit(self, X_train, y_train): is used to train the Gaussian Naive Bayes classifier. It calculates and stores the class probabilities, means, and variances for each feature in the training data.\n","3. predict(self, X_test): This method predicts the class labels for new data based on the trained model.\n","4. pdf(self, x, mean, var): This method calculates the probability density function (PDF) of a Gaussian distribution for a given data point x, mean, and variance."]},{"cell_type":"markdown","metadata":{},"source":["## PCA - Principle Component Analysis"]},{"cell_type":"markdown","metadata":{},"source":["1. **Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original variability as possible.**\n","1. **Dimensionality Reduction: PCA aims to reduce the number of features (dimensions) in a dataset while retaining as much information as possible. This is particularly useful when dealing with high-dimensional data where the number of features is large compared to the number of samples.**\n","1. **Orthogonal Transformation: PCA performs an orthogonal linear transformation to project the original data onto a new coordinate system defined by principal components. These components are orthogonal to each other and are ordered by the amount of variance they explain in the data.**\n","1. **Variance Maximization: The first principal component captures the direction of maximum variance in the data. Subsequent components capture the remaining variance in decreasing order, ensuring that the most important patterns in the data are retained in the lower-dimensional space.**\n","1. **Decorrelation: PCA also decorrelates the features in the transformed space, meaning that the new features (principal components) are uncorrelated with each other. This can be beneficial for certain machine learning algorithms that assume feature independence.**\n","1. **Eigenanalysis: PCA is based on eigenanalysis, where the eigenvectors of the covariance matrix of the data represent the directions of maximum variance, and the eigenvalues correspond to the amount of variance explained along those directions.**\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:40:58.265150Z","iopub.status.busy":"2024-04-19T16:40:58.264757Z","iopub.status.idle":"2024-04-19T16:40:58.279597Z","shell.execute_reply":"2024-04-19T16:40:58.277968Z","shell.execute_reply.started":"2024-04-19T16:40:58.265119Z"},"trusted":true},"outputs":[],"source":["class MyPCA:\n","    \n","    def __init__(self, n_components):\n","        self.n_components = n_components   \n","        \n","    def fit(self, X):\n","        # Standardize data \n","        X = X.copy()\n","        self.mean = np.mean(X, axis = 0)\n","        self.scale = np.std(X, axis = 0)\n","        X_std = (X - self.mean) / self.scale\n","        \n","        # Eigendecomposition of covariance matrix       \n","        cov_mat = np.cov(X_std.T)\n","        eig_vals, eig_vecs = np.linalg.eig(cov_mat) \n","        \n","        # Adjusting the eigenvectors that are largest in absolute value to be positive    \n","        max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n","        signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n","        eig_vecs = eig_vecs*signs[np.newaxis,:]\n","        eig_vecs = eig_vecs.T\n","       \n","        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n","        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n","        eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n","        eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n","        \n","        self.components = eig_vecs_sorted[:self.n_components,:]\n","        \n","        # Explained variance ratio\n","        self.explained_variance_ratio = [i/np.sum(eig_vals) for i in eig_vals_sorted[:self.n_components]]\n","        \n","        self.cum_explained_variance = np.cumsum(self.explained_variance_ratio)\n","\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        X_std = (X - self.mean) / self.scale\n","        X_proj = X_std.dot(self.components.T)\n","        \n","        return X_proj\n","    "]},{"cell_type":"markdown","metadata":{},"source":["1. This 'class MyPCA' Implements principal component analysis.\n","2. init(self, n_components): The constructor initializes the PCA object with the number of components (n_components) to retain after dimensionality reduction.\n","3. fit(self, X): This method fits the PCA model to the input data X and computes the principal components.\n","    1. Standardize data \n","    2. Eigendecomposition of covariance matrix \n","    3. Adjusting the eigenvectors that are largest in absolute value to be positive \n","    4. Explained variance ratio\n","4. transform(self, X): This method transforms the input data X into the reduced dimensional space using the computed principal components."]},{"cell_type":"markdown","metadata":{},"source":["### Displaying PCA results"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:41:01.848292Z","iopub.status.busy":"2024-04-19T16:41:01.847869Z","iopub.status.idle":"2024-04-19T16:41:01.888975Z","shell.execute_reply":"2024-04-19T16:41:01.887618Z","shell.execute_reply.started":"2024-04-19T16:41:01.848259Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Components:\n"," [[-0.30219096  0.64378667  0.62260719 -0.21242839 -0.06848339 -0.22694272\n","  -0.07253163]\n"," [ 0.33410693  0.03435809  0.2838292   0.35948683  0.73791663 -0.22065738\n","   0.290158  ]\n"," [-0.11204501 -0.10993913 -0.1631733  -0.24822796 -0.21359908 -0.54852029\n","   0.73526701]\n"," [-0.54165059 -0.04629318 -0.15486709  0.69082649 -0.0671714  -0.39570047\n","  -0.20531846]\n"," [-0.50778466  0.08233115  0.03342452  0.15486542  0.12887133  0.65188053\n","   0.51838188]\n"," [-0.48290443 -0.376847   -0.02896707 -0.50041798  0.54787098 -0.12571195\n","  -0.23992979]]\n","Explained variance ratio :\n"," [0.27588831430207345, 0.18484431095364603, 0.153787037552692, 0.1461273083991125, 0.1151326253047569, 0.09665166249057841]\n","Cumulative explained variance :\n"," [0.27588831 0.46073263 0.61451966 0.76064697 0.8757796  0.97243126]\n","Transformed data shape : (2200, 6)\n"]}],"source":["my_pca = MyPCA(n_components = 6).fit(X)\n","\n","print('Components:\\n', my_pca.components)\n","print('Explained variance ratio :\\n', my_pca.explained_variance_ratio)\n","print('Cumulative explained variance :\\n', my_pca.cum_explained_variance)\n","\n","X_proj = my_pca.transform(X)\n","print('Transformed data shape :', X_proj.shape)"]},{"cell_type":"markdown","metadata":{},"source":["1. Prints the principal components calculated by the PCA\n","2. Prints the explained variance ratio for each of the selected principal components. This ratio indicates the proportion of variance explained by each component relative to the total variance.\n","3.  Prints the cumulative explained variance, which shows the cumulative proportion of variance explained as more principal components are included.\n","4. ransforms the original data X into the lower-dimensional space using the computed principal components.\n","5. Prints the shape of the transformed data, which represents the reduced-dimensional representation of X after PCA."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:41:16.815825Z","iopub.status.busy":"2024-04-19T16:41:16.815474Z","iopub.status.idle":"2024-04-19T16:41:16.823328Z","shell.execute_reply":"2024-04-19T16:41:16.821640Z","shell.execute_reply.started":"2024-04-19T16:41:16.815797Z"},"trusted":true},"outputs":[],"source":["x_train,y_train,x_test,y_test=train_test_split(seed=20,split=0.2,X=X_proj,Y=Y)"]},{"cell_type":"markdown","metadata":{},"source":["## printing gaussian probabilities of each class"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:41:20.448064Z","iopub.status.busy":"2024-04-19T16:41:20.447636Z","iopub.status.idle":"2024-04-19T16:41:20.957712Z","shell.execute_reply":"2024-04-19T16:41:20.956274Z","shell.execute_reply.started":"2024-04-19T16:41:20.448033Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.048863636363636366\n","0.04375\n","0.045454545454545456\n","0.04431818181818182\n","0.045454545454545456\n","0.04318181818181818\n","0.04261363636363636\n","0.042045454545454546\n","0.04715909090909091\n","0.048295454545454544\n","0.045454545454545456\n","0.03977272727272727\n","0.045454545454545456\n","0.045454545454545456\n","0.04772727272727273\n","0.04375\n","0.04602272727272727\n","0.044886363636363634\n","0.048295454545454544\n","0.04602272727272727\n","0.04772727272727273\n","0.048295454545454544\n"]}],"source":["model = GaussianNaiveBayes()\n","model.fit(x_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Accuracy on prediction using GNB"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:41:58.222813Z","iopub.status.busy":"2024-04-19T16:41:58.222317Z","iopub.status.idle":"2024-04-19T16:41:58.711740Z","shell.execute_reply":"2024-04-19T16:41:58.710102Z","shell.execute_reply.started":"2024-04-19T16:41:58.222774Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9204545454545454\n"]}],"source":["predictions = model.predict(x_test)\n","correct_predictions=0\n","for pred, true_label in zip(predictions, y_test):\n","    if pred == true_label:\n","        correct_predictions += 1\n","accuracy=correct_predictions/len(y_test)\n","print(accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["## Precision- recall for multiclass data"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:43:11.937236Z","iopub.status.busy":"2024-04-19T16:43:11.936823Z","iopub.status.idle":"2024-04-19T16:43:11.967238Z","shell.execute_reply":"2024-04-19T16:43:11.966094Z","shell.execute_reply.started":"2024-04-19T16:43:11.937206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: Precision=1.0000, Recall=1.0000\n","Class 1: Precision=1.0000, Recall=1.0000\n","Class 2: Precision=0.8636, Recall=0.9500\n","Class 3: Precision=1.0000, Recall=1.0000\n","Class 4: Precision=0.9524, Recall=1.0000\n","Class 5: Precision=1.0000, Recall=1.0000\n","Class 6: Precision=0.8621, Recall=1.0000\n","Class 7: Precision=1.0000, Recall=1.0000\n","Class 8: Precision=0.8500, Recall=1.0000\n","Class 9: Precision=1.0000, Recall=1.0000\n","Class 10: Precision=0.8000, Recall=0.8000\n","Class 11: Precision=1.0000, Recall=0.8667\n","Class 12: Precision=0.9048, Recall=0.9500\n","Class 13: Precision=0.8462, Recall=0.5500\n","Class 14: Precision=0.9412, Recall=1.0000\n","Class 15: Precision=1.0000, Recall=1.0000\n","Class 16: Precision=0.8333, Recall=0.7895\n","Class 17: Precision=0.9474, Recall=0.8571\n","Class 18: Precision=0.6667, Recall=0.8000\n","Class 19: Precision=0.8421, Recall=0.8421\n","Class 20: Precision=0.8667, Recall=0.8125\n","Class 21: Precision=1.0000, Recall=1.0000\n"]}],"source":["def calculate_precision_recall_multi_class(true_labels, predicted_labels, num_classes):\n","    # Convert labels to numpy arrays for easier manipulation\n","    true_labels = np.array(true_labels)\n","    predicted_labels = np.array(predicted_labels)\n","    \n","    # Initialize dictionaries to store true positives, false positives, false negatives for each class\n","    true_positives = {}\n","    false_positives = {}\n","    false_negatives = {}\n","    \n","    # Calculate true positives, false positives, false negatives for each class\n","    for cls in range(num_classes):\n","        true_positives[cls] = sum((true == cls) and (pred == cls) for true, pred in zip(true_labels, predicted_labels))\n","        false_positives[cls] = sum((true != cls) and (pred == cls) for true, pred in zip(true_labels, predicted_labels))\n","        false_negatives[cls] = sum((true == cls) and (pred != cls) for true, pred in zip(true_labels, predicted_labels))\n","    \n","    # Calculate precision and recall for each class\n","    precision_recall = {}\n","    for cls in range(num_classes):\n","        precision = true_positives[cls] / (true_positives[cls] + false_positives[cls]) \\\n","            if true_positives[cls] + false_positives[cls] > 0 else 0\n","        recall = true_positives[cls] / (true_positives[cls] + false_negatives[cls]) \\\n","            if true_positives[cls] + false_negatives[cls] > 0 else 0\n","        precision_recall[cls] = (precision, recall)\n","    \n","    return precision_recall\n","\n","num_classes = 22  # Assuming classes range from 0 to 21\n","precision_recall = calculate_precision_recall_multi_class(y_test, predictions, num_classes)\n","\n","# Print precision and recall for each class\n","for cls, (precision, recall) in precision_recall.items():\n","    print(f'Class {cls}: Precision={precision:.4f}, Recall={recall:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["1. This code snippet helps in calculating precision-recall for mutli class classification problem\n","2. Data Preparation: Converts the true labels (true_labels) and predicted labels (predicted_labels) to NumPy arrays for easier manipulation.\n","3. Initialization: Initializes dictionaries (true_positives, false_positives, false_negatives) to store the counts of true positives, false positives, and false negatives for each class.\n","4. calculating counts of true positives, false positives, false negatives\n","5. Calculating Precision and Recall: \n","    1. Precision for class cls is calculated as true_positives[cls] / (true_positives[cls] + false_positives[cls]), handling cases where the denominator is zero.\n","    2. Recall for class cls is calculated as true_positives[cls] / (true_positives[cls] + false_negatives[cls]), handling cases where the denominator is zero.\n","6. returns the precision and recall results"]},{"cell_type":"markdown","metadata":{},"source":["## F1-score calculation"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:43:17.077237Z","iopub.status.busy":"2024-04-19T16:43:17.076861Z","iopub.status.idle":"2024-04-19T16:43:17.087394Z","shell.execute_reply":"2024-04-19T16:43:17.085764Z","shell.execute_reply.started":"2024-04-19T16:43:17.077207Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: F1 Score=1.0000\n","Class 1: F1 Score=1.0000\n","Class 2: F1 Score=0.9048\n","Class 3: F1 Score=1.0000\n","Class 4: F1 Score=0.9756\n","Class 5: F1 Score=1.0000\n","Class 6: F1 Score=0.9259\n","Class 7: F1 Score=1.0000\n","Class 8: F1 Score=0.9189\n","Class 9: F1 Score=1.0000\n","Class 10: F1 Score=0.8000\n","Class 11: F1 Score=0.9286\n","Class 12: F1 Score=0.9268\n","Class 13: F1 Score=0.6667\n","Class 14: F1 Score=0.9697\n","Class 15: F1 Score=1.0000\n","Class 16: F1 Score=0.8108\n","Class 17: F1 Score=0.9000\n","Class 18: F1 Score=0.7273\n","Class 19: F1 Score=0.8421\n","Class 20: F1 Score=0.8387\n","Class 21: F1 Score=1.0000\n"]}],"source":["def calculate_f1_score_multi_class(precision_recall):\n","    f1_scores = {}\n","    for cls, (precision, recall) in precision_recall.items():\n","        f1_scores[cls] = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n","    return f1_scores\n","\n","# Assuming you have already calculated precision_recall using calculate_precision_recall_multi_class\n","f1_scores = calculate_f1_score_multi_class(precision_recall)\n","\n","# Print F1 score for each class\n","for cls, f1 in f1_scores.items():\n","    print(f'Class {cls}: F1 Score={f1:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["1. The F1 score is a metric that combines precision and recall into a single value, providing a balanced assessment of a classifier's performance, especially in situations where there is an imbalance between classes or when both false positives and false negatives are important considerations.\n","2. F1 = 2*(precision * recall) / (precision + recall)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Overall metrics values of precision, recall, F1 score, accuracy"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:43:20.619833Z","iopub.status.busy":"2024-04-19T16:43:20.619271Z","iopub.status.idle":"2024-04-19T16:43:20.633669Z","shell.execute_reply":"2024-04-19T16:43:20.631916Z","shell.execute_reply.started":"2024-04-19T16:43:20.619791Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Precision: 0.9171054024507154\n","Overall Recall: 0.9189949305080883\n","Overall F1 Score: 0.9152672417087657\n"," Accuracy :  0.9204545454545454\n"]}],"source":["def calculate_overall_metrics(precision_recall, f1_scores):\n","    # Calculate overall precision, recall, and F1 score\n","    overall_precision = sum(precision for precision, _ in precision_recall.values()) / len(precision_recall)\n","    overall_recall = sum(recall for _, recall in precision_recall.values()) / len(precision_recall)\n","    overall_f1 = sum(f1 for f1 in f1_scores.values()) / len(f1_scores)\n","    return overall_precision, overall_recall, overall_f1\n","\n","# Assuming you have already calculated precision_recall and f1_scores\n","overall_precision, overall_recall, overall_f1 = calculate_overall_metrics(precision_recall, f1_scores)\n","\n","print('Overall Precision:', overall_precision)\n","print('Overall Recall:', overall_recall)\n","print('Overall F1 Score:', overall_f1)\n","print(' Accuracy : ', accuracy)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1046158,"sourceId":1760012,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
